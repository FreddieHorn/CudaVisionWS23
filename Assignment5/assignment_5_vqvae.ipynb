{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import torch.nn as nn\n",
    "from PIL import Image, ImageOps\n",
    "import torch\n",
    "import pdb\n",
    "import numpy as np\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append('..')\n",
    "from template import utils\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting config\n",
    "config = yaml.safe_load(open(\"config.yaml\"))\n",
    "batch_size = int(config[\"BATCH_SIZE\"])\n",
    "\n",
    "print(f\"Our config: {config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    v2.Resize((128, 128))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.CelebA(root='./data', split='train',\n",
    "                                        download=True, transform=transform)\n",
    "valid_dataset = torchvision.datasets.CelebA(root='./data', split='valid',\n",
    "                                       download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CelebA(root='./data', split='test',\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "#create dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display images from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(iter(testloader))\n",
    "print(f\"Image Shapes: {imgs.shape}\")\n",
    "print(f\"Label Shapes: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_IMGS = 8\n",
    "fig, ax = plt.subplots(1,N_IMGS)\n",
    "fig.set_size_inches(3 * N_IMGS, 3)\n",
    "\n",
    "ids = np.random.randint(low=0, high=len(train_dataset), size=N_IMGS)\n",
    "\n",
    "for i, n in enumerate(ids):\n",
    "    img = train_dataset[n][0].numpy().reshape(3,128,128).transpose(1, 2, 0)\n",
    "    ax[i].imshow(img)\n",
    "    #ax[i].set_title(f\"Img #{n}  Label: {train_dataset[n][1]}\")\n",
    "    #ax[i].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, epoch, stats, exp_no = 7):\n",
    "    \"\"\" Saving model checkpoint \"\"\"\n",
    "    \n",
    "    if(not os.path.exists(\"experiments/experiment_\"+str(exp_no)+\"/models\")):\n",
    "        os.makedirs(\"experiments/experiment_\"+str(exp_no)+\"/models\")\n",
    "    savepath = \"experiments/experiment_\"+str(exp_no)+f\"/models/checkpoint_epoch_{epoch}.pth\"\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'stats': stats\n",
    "    }, savepath)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def load_model(model, optimizer, savepath):\n",
    "    \"\"\" Loading pretrained checkpoint \"\"\"\n",
    "    \n",
    "    checkpoint = torch.load(savepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "    stats = checkpoint[\"stats\"]\n",
    "    \n",
    "    return model, optimizer, epoch, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion, epoch, device):\n",
    "    \"\"\" Training a model for one epoch \"\"\"\n",
    "    \n",
    "    loss_list = []\n",
    "    recons_loss = []\n",
    "    vae_loss = []\n",
    "    \n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for i, (images, _) in progress_bar:\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "         \n",
    "        # Forward pass\n",
    "        recons, quant_loss = model(images)\n",
    "         \n",
    "        # Calculate Loss\n",
    "        loss, mse = criterion(recons, quant_loss, images)\n",
    "        loss_list.append(loss.item())\n",
    "        recons_loss.append(mse.item())\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "         \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        progress_bar.set_description(f\"Epoch {epoch+1} Iter {i+1}: loss {loss.item():.5f}. \")\n",
    "        \n",
    "    mean_loss = np.mean(loss_list)\n",
    "    \n",
    "    return mean_loss, loss_list\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(model, eval_loader, criterion, device, epoch=None, savefig=False, savepath=\"\", writer=None):\n",
    "    \"\"\" Evaluating the model for either validation or test \"\"\"\n",
    "    loss_list = []\n",
    "    recons_loss = []\n",
    "    \n",
    "    for i, (images, _) in enumerate(eval_loader):\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Forward pass \n",
    "        recons, quant_loss = model(images)\n",
    "                 \n",
    "        loss, mse = criterion(recons, quant_loss, images)\n",
    "        loss_list.append(loss.item())\n",
    "        recons_loss.append(mse.item())\n",
    "        \n",
    "        if(i==0 and savefig):\n",
    "            save_image(recons[:64].cpu(), os.path.join(savepath, f\"recons{epoch}.png\"))\n",
    "            \n",
    "    # Total correct predictions and loss\n",
    "    loss = np.mean(loss_list)\n",
    "    recons_loss = np.mean(recons_loss)\n",
    "\n",
    "    return loss, recons_loss\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, scheduler, criterion, train_loader, valid_loader,\n",
    "                num_epochs, savepath, writer, save_frequency=2):\n",
    "    \"\"\" Training a model for a given number of epochs\"\"\"\n",
    "    \n",
    "    train_loss = []\n",
    "    val_loss =  []\n",
    "    val_loss_recons =  []\n",
    "    val_loss_kld =  []\n",
    "    loss_iters = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "           \n",
    "        # validation epoch\n",
    "        model.eval()  # important for dropout and batch norms\n",
    "        log_epoch = (epoch % save_frequency == 0 or epoch == num_epochs - 1)\n",
    "        loss, recons_loss= eval_model(\n",
    "                model=model, eval_loader=valid_loader, criterion=criterion,\n",
    "                device=device, epoch=epoch, savefig=log_epoch, savepath=savepath,\n",
    "                writer=writer\n",
    "            )\n",
    "        val_loss.append(loss)\n",
    "        val_loss_recons.append(recons_loss)\n",
    "        \n",
    "        # training epoch\n",
    "        model.train()  # important for dropout and batch norms\n",
    "        mean_loss, cur_loss_iters = train_epoch(\n",
    "                model=model, train_loader=train_loader, optimizer=optimizer,\n",
    "                criterion=criterion, epoch=epoch, device=device\n",
    "            )\n",
    "        \n",
    "        # PLATEAU SCHEDULER\n",
    "        scheduler.step(val_loss[-1])\n",
    "        train_loss.append(mean_loss)\n",
    "        loss_iters = loss_iters + cur_loss_iters\n",
    "        \n",
    "        if(epoch % save_frequency == 0):\n",
    "            stats = {\n",
    "                \"train_loss\": train_loss,\n",
    "                \"valid_loss\": val_loss,\n",
    "                \"loss_iters\": loss_iters\n",
    "            }\n",
    "            save_model(model=model, optimizer=optimizer, epoch=epoch, stats=stats)\n",
    "        \n",
    "        if(log_epoch):\n",
    "            print(f\"    Train loss: {round(mean_loss, 5)}\")\n",
    "            print(f\"    Valid loss: {round(loss, 5)}\")\n",
    "            print(f\"       Valid loss recons: {round(val_loss_recons[-1], 5)}\")\n",
    "    \n",
    "    print(f\"Training completed\")\n",
    "    return train_loss, val_loss, loss_iters, val_loss_recons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropouts + additional conv layers led to brwon pictures, but more details -> did not improve. When only using kernel size = 2 we saw very blocky pictures so we opted for kernel size 3 with a smaller stride. The results were as expected. We lost some of the blockieness. Increasing encoder linear layers did not help   (led to brownness)\n",
    "After fixing this with additional conv operations ath the end, we still had the problem that the vae was overfitting to much to the input vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code was taken from: https://colab.research.google.com/github/zalandoresearch/pytorch-vq-vae/blob/master/vq-vae.ipynb\n",
    "# Note, that the commitment_cost is the beta parameter from the paper\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        \n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._num_embeddings = num_embeddings\n",
    "        \n",
    "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
    "        self._embedding.weight.data.uniform_(-1/self._num_embeddings, 1/self._num_embeddings)\n",
    "        self._commitment_cost = commitment_cost\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # convert inputs from BCHW -> BHWC\n",
    "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
    "        input_shape = inputs.shape\n",
    "        \n",
    "        # Flatten input\n",
    "        flat_input = inputs.view(-1, self._embedding_dim)\n",
    "        \n",
    "        # Calculate distances\n",
    "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) \n",
    "                    + torch.sum(self._embedding.weight**2, dim=1)\n",
    "                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n",
    "            \n",
    "        # Encoding\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "        \n",
    "        # Quantize and unflatten\n",
    "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
    "        \n",
    "        # Loss\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
    "        q_latent_loss = F.mse_loss(quantized, inputs.detach())\n",
    "        loss = q_latent_loss + self._commitment_cost * e_latent_loss\n",
    "        \n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "        \n",
    "        # convert quantized from BHWC -> BCHW\n",
    "        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings\n",
    "\n",
    "\n",
    "embedding_dim = 64\n",
    "num_embeddings = 512\n",
    "\n",
    "commitment_cost = 0.25\n",
    "\n",
    "VQ = VectorQuantizer(embedding_dim=embedding_dim, num_embeddings=num_embeddings, commitment_cost=commitment_cost)\n",
    "\n",
    "inp = torch.zeros(64, 64, 8, 8)\n",
    "\n",
    "_, out, _, _= VQ(inp)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Code was heavily inspired by Explaining AI: https://www.youtube.com/watch?v=1ZHzAOutcnw\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VQVAE, self).__init__()\n",
    "        \n",
    "        # Define Convolutional Encoders\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        VQ = VectorQuantizer(embedding_dim=64, num_embeddings=512, commitment_cost=0.25)\n",
    "\n",
    "        # Define decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels = 64, out_channels = 32, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(), \n",
    "            nn.ConvTranspose2d(in_channels = 32, out_channels = 16, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.Tanh(),\n",
    "            nn.ConvTranspose2d(in_channels = 16, out_channels = 3, kernel_size = 3, stride = 1, padding = 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded_output = self.encoder(x)\n",
    "        quant_input = self.pre_quant_conv(encoded_output)\n",
    "\n",
    "        ## Quantization\n",
    "\n",
    "        B, C, H, W = quant_input.shape\n",
    "        quant_input = quant_input.permute(0, 2, 3, 1)\n",
    "        # Shape: B, Pixels, C\n",
    "        quant_input = quant_input.reshape(quant_input.size(0), -1, quant_input.size(-1))\n",
    "\n",
    "        # Reshape for torch.cdist\n",
    "        weights = self.embedding.weight[None, :].repeat(quant_input.size(0), 1, 1).permute(0, 2, 1)\n",
    "        # Compute pairwise distances\n",
    "        # print(quant_input.shape)\n",
    "        # dist = torch.cdist(quant_input, self.embedding.weight[None, :].repeat(quant_input.size(0), 1, 1))\n",
    "        dist = torch.cdist(quant_input, weights)\n",
    "\n",
    "        # print(dist.shape)\n",
    "        # Find index of nearest embedding\n",
    "        min_encoding_indices = torch.argmin(dist, dim=-1)\n",
    "\n",
    "        # Select the embedding weights\n",
    "        print(quant_input.shape)\n",
    "        print(min_encoding_indices.shape)\n",
    "        print(self.embedding.weight.shape)\n",
    "        quant_out = torch.index_select(self.embedding.weight.repeat(quant_input.size(0), 1, 1), 2, min_encoding_indices.view(-1))\n",
    "        print(quant_out.shape)\n",
    "        quant_input = quant_input.reshape((-1, quant_input.size(-1)))\n",
    "\n",
    "        # Compute losses (mean or some does not change the gradient only the gradient length)\n",
    "        commitment_loss = torch.mean((quant_out.detach() - quant_input)**2)\n",
    "        codebook_loss = torch.mean((quant_out - quant_input.detach()**2))\n",
    "        quantize_losses = codebook_loss + self.beta*commitment_loss\n",
    "\n",
    "        # Reshape the output\n",
    "        quant_out = quant_input + (quant_out - quant_input).detach()\n",
    "        quant_out = quant_out.reshape((B, H, W, C)).permute(0, 3, 1, 2)\n",
    "        min_encoding_indices = min_encoding_indices.reshape((-1, quant_out.size(-1), quant_out))\n",
    "\n",
    "        # Decoding\n",
    "\n",
    "        decoder_input = self.post_quant_conv(quant_out)\n",
    "        output = self.decoder(decoder_input)\n",
    "\n",
    "        return output, quantize_losses\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VQVAE, self).__init__()\n",
    "        \n",
    "        # Define Convolutional Encoders\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size = 4, stride = 2, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 4, stride = 2, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 4, stride = 2, padding = 1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.VQ = VectorQuantizer(embedding_dim=64, num_embeddings=512, commitment_cost=0.25)\n",
    "\n",
    "        # Define decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels = 64, out_channels = 32, kernel_size = 4, stride = 2, padding = 1),\n",
    "            nn.ReLU(), \n",
    "            nn.ConvTranspose2d(in_channels = 32, out_channels = 16, kernel_size = 4, stride = 2, padding = 1),\n",
    "            nn.Tanh(),\n",
    "            nn.ConvTranspose2d(in_channels = 16, out_channels = 3, kernel_size = 4, stride = 2, padding = 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        q_loss, x, ppl, encodings = self.VQ(x)\n",
    "        out = self.decoder(x) \n",
    "\n",
    "        return out, q_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vqvae_loss_function(recons, quantize_losses, target):\n",
    "    recons_loss = F.mse_loss(recons, target)\n",
    "    \n",
    "    loss = recons_loss + quantize_losses\n",
    "\n",
    "    return loss, quantize_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "vqvae = VQVAE()\n",
    "criterion = vqvae_loss_function\n",
    "optimizer = torch.optim.Adam(vqvae.parameters(), lr=3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = 3, factor = 0.5, verbose = True)\n",
    "vqvae = vqvae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = \"/home/user/lschulze/projects/CudaVisionWS23/Assignment5/experiments/experiment_7\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, val_loss, loss_iters, val_loss_recons = train_model(\n",
    "        model=vqvae, optimizer=optimizer, scheduler=scheduler, criterion=vqvae_loss_function,\n",
    "        train_loader=trainloader, valid_loader=validloader, num_epochs=20, savepath=savepath, writer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        z = torch.randn(64, 200).to(device)\n",
    "        sample = cvae.decoder(z)\n",
    "    \n",
    "\n",
    "recons = sample.view(64, 3, 128, 128).cpu()\n",
    "\n",
    "recons.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recons.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 10, figsize=(128, 128))  # Adjust figsize as needed\n",
    "\n",
    "for i in range(10):\n",
    "    img = recons[i].numpy().reshape(3, 128, 128).transpose(1, 2, 0)\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].axis('off')  # Turn off axis labels for clarity\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[i][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, labels = next(iter(testloader))\n",
    "test_data = test_data.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample, _ = cvae(test_data)\n",
    "\n",
    "    recons = sample.view(batch_size, 3, 128, 128).cpu()\n",
    "\n",
    "    fig, axes = plt.subplots(2, 10, figsize=(128, 128))  # Adjust figsize as needed\n",
    "\n",
    "    for i in range(10):\n",
    "        img = recons[i].numpy().reshape(3, 128, 128).transpose(1, 2, 0)\n",
    "        # test_img = test_data[i].reshape(3, 128, 128).transpose(1, 2, 0)\n",
    "        test_img = test_data[i].cpu().numpy().reshape(3,128,128).transpose(1, 2, 0)\n",
    "        axes[0][i].imshow(test_img)\n",
    "        axes[0][i].axis('off')\n",
    "        axes[1][i].imshow(img)\n",
    "        axes[1][i].axis('off')  # Turn off axis labels for clarity\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frechet Inception Distance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
