{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee6ddc65-0b5d-40e8-b7c7-986cf4d03055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pdb\n",
    "import numpy as np\n",
    "import yaml\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "436b1e3f-a3be-4ed7-bbec-560ab7b5519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the subjects for each split\n",
    "train_subjects = [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 3, 5, 6, 7, 10]\n",
    "val_subjects = [24, 25, 1, 4]\n",
    "test_subjects = [22, 2, 8, 9]\n",
    "\n",
    "# Define the background variations\n",
    "background_variations = ['d1', 'd2', 'd3', 'd4']\n",
    "\n",
    "processed_folder = './processed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bea475fe-d0bf-45b5-9538-9c0614a3ef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load config file and hyperparams\n",
    "config = yaml.safe_load(open(\"config.yaml\"))\n",
    "LR = float(config[\"LR\"])\n",
    "batch_size = int(config[\"BATCH_SIZE\"])\n",
    "num_epochs = int(config[\"NUM_EPOCHS\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52427c9c-27c0-4301-b52e-6c0d8cfcd917",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    # Add more augmentations if needed\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "636c9779-c8cc-441d-802d-df82f6ee47a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2a56bfe-c3af-4608-9019-caf7bc52dbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequencesExtractor:\n",
    "    def __init__(self, path, num_frames_per_subsequence=20):\n",
    "        # Define the number of frames per subsequence\n",
    "        self.num_frames_per_subsequence = num_frames_per_subsequence\n",
    "        # List all action folders in the processed folder\n",
    "        self.classes = os.listdir(path) # folders correspond to classes/labels\n",
    "        self.class_to_label = {class_name: idx for idx, class_name in enumerate(self.classes)}\n",
    "    \n",
    "    def get_classes(self):\n",
    "        return self.classes\n",
    "    def create_sequences(self, subjects, background_variations):\n",
    "        sequences = []\n",
    "        target_arr = []\n",
    "        # Iterate over each action folder\n",
    "        for action_folder in self.classes:\n",
    "            target = action_folder\n",
    "            action_path = os.path.join(processed_folder, action_folder)\n",
    "\n",
    "            # List all person folders in the action folder\n",
    "            person_folders_actual = os.listdir(action_path)\n",
    "            # Filter videos based on subjects and background variations\n",
    "            person_folders_target = [f'person{subject:02d}_{action_folder}_{bg}' \n",
    "                           for subject in subjects \n",
    "                           for bg in background_variations]\n",
    "            person_folders = set(person_folders_actual) & set(person_folders_target) # it can be that some background variations (or smth else) is missing. \n",
    "            # Iterate over each person folder\n",
    "            for person_folder in person_folders:\n",
    "                person_path = os.path.join(action_path, person_folder)\n",
    "                # List all image files in the person folder\n",
    "                image_files = sorted(os.listdir(person_path))\n",
    "\n",
    "                # Split the image files into subsequences\n",
    "                num_frames = len(image_files)\n",
    "                num_subsequences = num_frames // self.num_frames_per_subsequence\n",
    "\n",
    "                for i in range(num_subsequences):\n",
    "                    start_index = i * self.num_frames_per_subsequence\n",
    "                    end_index = start_index + self.num_frames_per_subsequence\n",
    "\n",
    "                    # Load and process the frames in the subsequence\n",
    "                    subsequence_frames = []\n",
    "                    for j in range(start_index, end_index):\n",
    "                        try:\n",
    "                            image_path = os.path.join(person_path, image_files[j])\n",
    "                            frame = Image.open(image_path).convert('RGB')\n",
    "                            # Apply any desired spatial augmentations to the frame\n",
    "                            frame = transform(frame)\n",
    "                            subsequence_frames.append(frame)\n",
    "                        except:\n",
    "                            print(\"Tried to read wrong file. Continuing\")\n",
    "                            continue\n",
    "\n",
    "                  #  subsequence_frames = torch.stack(subsequence_frames, dim=0)\n",
    "                    # Perform further processing on the subsequence\n",
    "                    # Check if subsequence has the expected number of frames\n",
    "                    if len(subsequence_frames) == self.num_frames_per_subsequence:\n",
    "                        # Process the subsequence (e.g., feed it to a model for action classification)\n",
    "                        subsequence_frames = torch.stack(subsequence_frames, dim=0)\n",
    "                        # Perform further processing on the subsequence\n",
    "\n",
    "                        target_arr.append(self.class_to_label[target])\n",
    "                        sequences.append(subsequence_frames)\n",
    "                    else:\n",
    "                        print(f\"Skipping subsequence due to incorrect number of frames: {len(subsequence_frames)}\")\n",
    "                        # target_arr.append(self.class_to_label[target])\n",
    "                        # sequences.append(subsequence_frames)\n",
    "        return np.array(sequences), np.array(target_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f533d15-a84d-43a8-84ad-a38e23899726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to read wrong file. Continuing\n",
      "Skipping subsequence due to incorrect number of frames: 19\n",
      "Tried to read wrong file. Continuing\n",
      "Skipping subsequence due to incorrect number of frames: 19\n",
      "Tried to read wrong file. Continuing\n",
      "Skipping subsequence due to incorrect number of frames: 19\n",
      "Tried to read wrong file. Continuing\n",
      "Skipping subsequence due to incorrect number of frames: 19\n",
      "Tried to read wrong file. Continuing\n",
      "Skipping subsequence due to incorrect number of frames: 19\n",
      "Tried to read wrong file. Continuing\n",
      "Skipping subsequence due to incorrect number of frames: 19\n",
      "Tried to read wrong file. Continuing\n",
      "Skipping subsequence due to incorrect number of frames: 19\n",
      "Tried to read wrong file. Continuing\n",
      "Skipping subsequence due to incorrect number of frames: 19\n",
      "Tried to read wrong file. Continuing\n",
      "Skipping subsequence due to incorrect number of frames: 19\n"
     ]
    }
   ],
   "source": [
    "sequencesExtractor = SequencesExtractor(path=processed_folder, num_frames_per_subsequence=20)\n",
    "train_sequences, train_target_arr = sequencesExtractor.create_sequences(train_subjects, background_variations)\n",
    "test_sequences, test_target_arr = sequencesExtractor.create_sequences(test_subjects, background_variations)\n",
    "val_sequences, val_target_arr = sequencesExtractor.create_sequences(val_subjects, background_variations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ede534a-b8e5-4f6d-94c4-33d2830fc1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sequences length: 9841\n",
      "Validation sequences length: 2140\n",
      "Test sequences length: 2252\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training sequences length: {len(train_sequences)}\")\n",
    "print(f\"Validation sequences length: {len(val_sequences)}\")\n",
    "print(f\"Test sequences length: {len(test_sequences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c5cb3d4-bc76-458a-a340-35b92a077e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28561b37-ee02-4478-aabc-12375160948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KTHDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return sequence, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9478965c-f0de-45e0-afd5-cc43f819cf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = KTHDataset(train_sequences, train_target_arr)\n",
    "test_dataset = KTHDataset(test_sequences, test_target_arr)\n",
    "val_dataset = KTHDataset(val_sequences, val_target_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "641f7753-a710-4a5a-8167-15f91e1bab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89d507ec-7265-495e-85b0-91906cd03275",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not os.path.exists(\"models\")):\n",
    "    os.makedirs(\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "420430c4-617e-44db-a5d1-9af2636b4134",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Encapuslation of a convolutional block (conv + activation + pooling)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, k_size, pool=False, dropout_prob = 0.2, mxpool_stride=1):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(in_ch, out_ch, k_size))\n",
    "        layers.append(nn.ReLU())\n",
    "        if(pool):\n",
    "            layers.append(nn.MaxPool2d(kernel_size=2))\n",
    "       # layers.append(nn.Dropout(p=dropout_prob))\n",
    "        self.module = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return(self.module(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41147836-3cb6-427f-9c02-0978314aee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvRecurrentClassifier(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_size, num_classes,num_layers = 20, mode=\"zeros\"):\n",
    "        assert mode in [\"zeros\", \"random\"]\n",
    "        super(ConvRecurrentClassifier, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_size\n",
    "        #Convolutional Encoder\n",
    "        # self.conv_encoder = nn.Sequential(\n",
    "        #     ConvBlock(3, 16, 3, pool=False),\n",
    "        #     ConvBlock(16, 32, 3, pool=True),\n",
    "        #     ConvBlock(32, 64, 3, pool=False),\n",
    "        #     ConvBlock(64, 128, 3, pool=True)\n",
    "        # )\n",
    "        \n",
    "        self.conv_encoder = nn.Sequential(\n",
    "            # nn.Conv2d(input_channels, 64, kernel_size=3, padding=1),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Add more convolutional layers if needed\n",
    "            ConvBlock(3, 16, 3, pool=False),\n",
    "            ConvBlock(16, 32, 3, pool=True),\n",
    "            ConvBlock(32, 64, 3, pool=False),\n",
    "            ConvBlock(64, 128, 3, pool=True),\n",
    "            ConvBlock(128, 256, 3, pool=False),\n",
    "            ConvBlock(256, 512, 3, pool=True),\n",
    "            ConvBlock(512, 1024, 3, pool=True),\n",
    "        )\n",
    "        \n",
    "        #Recurrent Module\n",
    "        #self.lstm = nn.LSTM(input_size=256 * 315 * 11, hidden_size=hidden_size, batch_first=True)\n",
    "        # nn.LSTM\n",
    "        # LSTM model       \n",
    "        lstms = []\n",
    "        for i in range(num_layers):\n",
    "            in_size = 78848 if i == 0 else self.hidden_dim\n",
    "            lstms.append( nn.LSTMCell(input_size=in_size, hidden_size=self.hidden_dim) )\n",
    "        self.lstm = nn.ModuleList(lstms)\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b_size, seq_length, n_channels, width, height = x.shape\n",
    "        # Reshape input to (batch_size, channels, sequence_length*height, width)\n",
    "        x = x.view(b_size, n_channels, seq_length*width, height)\n",
    "        \n",
    "        h, c = self.init_state(b_size=b_size, device=device)\n",
    "         # Convolutional Encoder\n",
    "        x = self.conv_encoder(x)\n",
    "        \n",
    "        # Reshape for LSTM\n",
    "        embeddings = x.view(x.size(0), -1, x.size(1) * x.size(2) * x.size(3))\n",
    "        # Recurrent Module\n",
    "        #out, (h_out, c_out) = self.lstm(x, (h,c))\n",
    "        # iterating over sequence length\n",
    "        lstm_out = []\n",
    "        for i in range(embeddings.shape[1]):\n",
    "            lstm_input = embeddings[:, i, :]\n",
    "            # iterating over LSTM Cells\n",
    "            for j, lstm_cell in enumerate(self.lstm):\n",
    "                h[j], c[j] = lstm_cell(lstm_input, (h[j], c[j]))\n",
    "                lstm_input = h[j]\n",
    "            lstm_out.append(lstm_input)\n",
    "        lstm_out = torch.stack(lstm_out, dim=1)\n",
    "        # Take the output from the last time step\n",
    "        out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Classifier\n",
    "        out = self.classifier(out)\n",
    "        \n",
    "        return out\n",
    "    def init_state(self, b_size, device):\n",
    "        \"\"\" Initializing hidden and cell state \"\"\"\n",
    "        if(self.mode == \"zeros\"):\n",
    "            h = [torch.zeros(b_size, self.hidden_dim).to(device) for _ in range(self.num_layers)]\n",
    "            c = [torch.zeros(b_size, self.hidden_dim).to(device) for _ in range(self.num_layers)]\n",
    "        elif(self.mode == \"random\"):\n",
    "            h = [torch.zeros(b_size, self.hidden_dim).to(device) for _ in range(self.num_layers)]\n",
    "            c = [torch.zeros(b_size, self.hidden_dim).to(device) for _ in range(self.num_layers)]\n",
    "        return h, c\n",
    "        # h = h.to(device)\n",
    "        # c = c.to(device)\n",
    "        # return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "214bc3e6-c062-4160-8434-ce233eb4c146",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = ConvRecurrentClassifier(input_channels=3, hidden_size=128, num_classes=len(sequencesExtractor.get_classes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18e970ba-de1a-42c7-8d2e-85b996ae331f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "criterion = nn.CrossEntropyLoss()  # Note, that this already includes a Softmax!\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR) #adamW was used in the paper\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train_dataloader), epochs=num_epochs)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5985ea7-ecdf-453c-ab27-c6ba83b9eff7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59a1a7ad-d327-482d-a73d-43461f5268f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_model(model):\n",
    "    \"\"\" Computing model accuracy \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_list = []\n",
    "    \n",
    "    for sequences, labels in val_dataloader:\n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass only to get logits/output\n",
    "        outputs = model(sequences)\n",
    "                 \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "            \n",
    "        # Get predictions from the maximum value\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct += len( torch.where(preds==labels)[0] )\n",
    "        total += len(labels)\n",
    "                 \n",
    "    # Total correct predictions and loss\n",
    "    accuracy = correct / total * 100\n",
    "    loss = np.mean(loss_list)\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33439054-0afc-447d-83dc-eac9fd746f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iter 308: loss 1.80868. : 100%|███████████████████████████████████████████████| 308/308 [00:30<00:00,  9.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy at epoch 0: 23.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Iter 308: loss 1.71787. : 100%|███████████████████████████████████████████████| 308/308 [00:31<00:00,  9.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy at epoch 1: 23.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Iter 308: loss 1.72162. : 100%|███████████████████████████████████████████████| 308/308 [00:31<00:00,  9.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy at epoch 2: 23.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Iter 308: loss 1.74556. : 100%|███████████████████████████████████████████████| 308/308 [00:31<00:00,  9.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy at epoch 3: 23.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Iter 308: loss 1.74241. : 100%|███████████████████████████████████████████████| 308/308 [00:31<00:00,  9.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy at epoch 4: 23.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Iter 308: loss 1.81384. : 100%|███████████████████████████████████████████████| 308/308 [00:32<00:00,  9.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy at epoch 5: 23.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 Iter 308: loss 1.71015. : 100%|███████████████████████████████████████████████| 308/308 [00:32<00:00,  9.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy at epoch 6: 23.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 Iter 308: loss 1.77283. : 100%|███████████████████████████████████████████████| 308/308 [00:32<00:00,  9.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy at epoch 7: 23.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 Iter 308: loss 1.76986. : 100%|███████████████████████████████████████████████| 308/308 [00:32<00:00,  9.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy at epoch 8: 23.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 Iter 308: loss 1.85641. : 100%|██████████████████████████████████████████████| 308/308 [00:32<00:00,  9.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy at epoch 9: 23.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 Iter 308: loss 1.71220. : 100%|██████████████████████████████████████████████| 308/308 [00:32<00:00,  9.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy at epoch 10: 23.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 Iter 308: loss 1.80133. : 100%|██████████████████████████████████████████████| 308/308 [00:32<00:00,  9.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy at epoch 11: 23.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 Iter 308: loss 1.78872. : 100%|██████████████████████████████████████████████| 308/308 [00:32<00:00,  9.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy at epoch 12: 23.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 Iter 285: loss 1.79094. :  93%|██████████████████████████████████████████▌   | 285/308 [00:30<00:02,  9.46it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     35\u001b[0m    \u001b[38;5;66;03m# scheduler.step()\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     progress_bar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Iter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m loss_hist\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(loss_list))\n\u001b[1;32m     40\u001b[0m train_acc_hist\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(acc_list))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#SAMPLE Training\n",
    "loss_hist = []\n",
    "train_acc_hist = []\n",
    "valid_acc_hist = []\n",
    "valid_loss_hist = []\n",
    "best_loss = 100\n",
    "for epoch in range(num_epochs):\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    progress_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader))\n",
    "    for i, (sequences, labels) in progress_bar:\n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(sequences)\n",
    "         \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predicted = outputs.argmax(dim=-1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            accuracy = correct/labels.shape[0] * 100\n",
    "        acc_list.append(accuracy)\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "       # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0, norm_type=2.0)\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "       # scheduler.step()\n",
    "        \n",
    "        progress_bar.set_description(f\"Epoch {epoch+1} Iter {i+1}: loss {loss.item():.5f}. \")\n",
    "    \n",
    "    loss_hist.append(np.mean(loss_list))\n",
    "    train_acc_hist.append(np.mean(acc_list))\n",
    "    val_accuracy, valid_loss = eval_model(model)\n",
    "    if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, f\"models/checkpoint_epoch_{epoch}.pth\")\n",
    "    print(f\"Val accuracy at epoch {epoch}: {round(val_accuracy, 2)}%\")\n",
    "    valid_loss_hist.append(valid_loss)\n",
    "    valid_acc_hist.append(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5f59aa-062a-4d2a-a0b5-1ddd0d8236fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8168a5-10ec-4cb3-aee0-cb70fe23e2cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
