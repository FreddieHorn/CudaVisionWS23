{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee6ddc65-0b5d-40e8-b7c7-986cf4d03055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pdb\n",
    "import numpy as np\n",
    "import yaml\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "436b1e3f-a3be-4ed7-bbec-560ab7b5519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the subjects for each split\n",
    "train_subjects = [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 3, 5, 6, 7, 10]\n",
    "val_subjects = [24, 25, 1, 4]\n",
    "test_subjects = [22, 2, 8, 9]\n",
    "\n",
    "# Define the background variations\n",
    "background_variations = ['d1', 'd2', 'd3', 'd4']\n",
    "\n",
    "processed_folder = './processed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bea475fe-d0bf-45b5-9538-9c0614a3ef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load config file and hyperparams\n",
    "config = yaml.safe_load(open(\"config.yaml\"))\n",
    "LR = float(config[\"LR\"])\n",
    "batch_size = int(config[\"BATCH_SIZE\"])\n",
    "num_epochs = int(config[\"NUM_EPOCHS\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52427c9c-27c0-4301-b52e-6c0d8cfcd917",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    # Add more augmentations if needed\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2a56bfe-c3af-4608-9019-caf7bc52dbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequencesExtractor:\n",
    "    def __init__(self, path, num_frames_per_subsequence=20):\n",
    "        # Define the number of frames per subsequence\n",
    "        self.num_frames_per_subsequence = num_frames_per_subsequence\n",
    "        # List all action folders in the processed folder\n",
    "        self.classes = os.listdir(path) # folders correspond to classes/labels\n",
    "        self.class_to_label = {class_name: idx for idx, class_name in enumerate(self.classes)}\n",
    "    \n",
    "    def get_classes(self):\n",
    "        return self.classes\n",
    "    def create_sequences(self, subjects, background_variations):\n",
    "        sequences = []\n",
    "        target_arr = []\n",
    "        # Iterate over each action folder\n",
    "        for action_folder in self.classes:\n",
    "            target = action_folder\n",
    "            action_path = os.path.join(processed_folder, action_folder)\n",
    "\n",
    "            # List all person folders in the action folder\n",
    "            person_folders_actual = os.listdir(action_path)\n",
    "            # Filter videos based on subjects and background variations\n",
    "            person_folders_target = [f'person{subject:02d}_{action_folder}_{bg}' \n",
    "                           for subject in subjects \n",
    "                           for bg in background_variations]\n",
    "            person_folders = set(person_folders_actual) & set(person_folders_target) # it can be that some background variations (or smth else) is missing. \n",
    "            # Iterate over each person folder\n",
    "            for person_folder in person_folders:\n",
    "                person_path = os.path.join(action_path, person_folder)\n",
    "                # List all image files in the person folder\n",
    "                image_files = sorted(os.listdir(person_path))\n",
    "\n",
    "                # Split the image files into subsequences\n",
    "                num_frames = len(image_files)\n",
    "                num_subsequences = num_frames // self.num_frames_per_subsequence\n",
    "\n",
    "                for i in range(num_subsequences):\n",
    "                    start_index = i * self.num_frames_per_subsequence\n",
    "                    end_index = start_index + self.num_frames_per_subsequence\n",
    "\n",
    "                    # Load and process the frames in the subsequence\n",
    "                    subsequence_frames = []\n",
    "                    for j in range(start_index, end_index):\n",
    "                        try:\n",
    "                            image_path = os.path.join(person_path, image_files[j])\n",
    "                            frame = Image.open(image_path).convert('RGB')\n",
    "                            # Apply any desired spatial augmentations to the frame\n",
    "                            frame = transform(frame)\n",
    "                            subsequence_frames.append(frame)\n",
    "                        except:\n",
    "                            print(\"Tried to read wrong file. Continuing\")\n",
    "                            continue\n",
    "                    # Apply any desired temporal augmentations to the subsequence\n",
    "\n",
    "                    # Process the subsequence (e.g., feed it to a model for action classification)\n",
    "                    subsequence_frames = torch.stack(subsequence_frames, dim=0)\n",
    "                    # Perform further processing on the subsequence\n",
    "\n",
    "                    target_arr.append(self.class_to_label[target])\n",
    "                    sequences.append(subsequence_frames)\n",
    "        return np.array(sequences), np.array(target_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f533d15-a84d-43a8-84ad-a38e23899726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to read wrong file. Continuing\n",
      "Tried to read wrong file. Continuing\n",
      "Tried to read wrong file. Continuing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-201df742e6b2>:60: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  return np.array(sequences), np.array(target_arr)\n",
      "<ipython-input-6-201df742e6b2>:60: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(sequences), np.array(target_arr)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to read wrong file. Continuing\n",
      "Tried to read wrong file. Continuing\n",
      "Tried to read wrong file. Continuing\n",
      "Tried to read wrong file. Continuing\n",
      "Tried to read wrong file. Continuing\n",
      "Tried to read wrong file. Continuing\n"
     ]
    }
   ],
   "source": [
    "sequencesExtractor = SequencesExtractor(path=processed_folder, num_frames_per_subsequence=20)\n",
    "train_sequences, train_target_arr = sequencesExtractor.create_sequences(train_subjects, background_variations)\n",
    "test_sequences, test_target_arr = sequencesExtractor.create_sequences(test_subjects, background_variations)\n",
    "val_sequences, val_target_arr = sequencesExtractor.create_sequences(val_subjects, background_variations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ede534a-b8e5-4f6d-94c4-33d2830fc1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sequences length: 9844\n",
      "Validation sequences length: 2146\n",
      "Test sequences length: 2252\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training sequences length: {len(train_sequences)}\")\n",
    "print(f\"Validation sequences length: {len(val_sequences)}\")\n",
    "print(f\"Test sequences length: {len(test_sequences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c5cb3d4-bc76-458a-a340-35b92a077e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28561b37-ee02-4478-aabc-12375160948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KTHDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return sequence, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a513b5d-6569-4213-872d-e1982b6bdf14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9478965c-f0de-45e0-afd5-cc43f819cf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = KTHDataset(train_sequences, train_target_arr)\n",
    "test_dataset = KTHDataset(test_sequences, test_target_arr)\n",
    "val_dataset = KTHDataset(val_sequences, val_target_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a23c7c6e-4cb1-4889-ac54-456a848e0e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.KTHDataset at 0x7f56b7687580>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "641f7753-a710-4a5a-8167-15f91e1bab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d507ec-7265-495e-85b0-91906cd03275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "420430c4-617e-44db-a5d1-9af2636b4134",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Encapuslation of a convolutional block (conv + activation + pooling)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, k_size, pool=False):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(in_ch, out_ch, k_size))\n",
    "        layers.append(nn.ReLU())\n",
    "        if(pool):\n",
    "            layers.append(nn.MaxPool2d(kernel_size=2))\n",
    "        self.module = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return(self.module(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41147836-3cb6-427f-9c02-0978314aee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvRecurrentClassifier(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_size, num_classes,num_layers = 1, mode=\"zeros\"):\n",
    "        assert mode in [\"zeros\", \"random\"]\n",
    "        super(ConvRecurrentClassifier, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.num_layers = 1\n",
    "        self.hidden_dim = hidden_size\n",
    "        #Convolutional Encoder\n",
    "        # self.conv_encoder = nn.Sequential(\n",
    "        #     ConvBlock(3, 16, 3, pool=False),\n",
    "        #     ConvBlock(16, 32, 3, pool=True),\n",
    "        #     ConvBlock(32, 64, 3, pool=False),\n",
    "        #     ConvBlock(64, 128, 3, pool=True)\n",
    "        # )\n",
    "        \n",
    "        self.conv_encoder = nn.Sequential(\n",
    "            # nn.Conv2d(input_channels, 64, kernel_size=3, padding=1),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Add more convolutional layers if needed\n",
    "            ConvBlock(3, 16, 3, pool=False),\n",
    "            ConvBlock(16, 32, 3, pool=True),\n",
    "            ConvBlock(32, 64, 3, pool=False),\n",
    "            ConvBlock(64, 128, 3, pool=True),\n",
    "            ConvBlock(128, 256, 3, pool=False)\n",
    "        )\n",
    "        \n",
    "        #Recurrent Module\n",
    "        self.lstm = nn.LSTM(input_size=256 * 315 * 11, hidden_size=hidden_size, batch_first=True)\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Reshape input to (batch_size * sequence_length, channels, height, width)\n",
    "        b_size, seq_length, n_channels, width, height = x.shape\n",
    "        \n",
    "        x = x.view(b_size, n_channels, seq_length*width, height)\n",
    "        \n",
    "        h, c = self.init_state(b_size=batch_size, device=device)\n",
    "         # Convolutional Encoder\n",
    "        x = self.conv_encoder(x)\n",
    "        \n",
    "        # Reshape for LSTM\n",
    "        x = x.view(x.size(0), -1, x.size(1) * x.size(2) * x.size(3))\n",
    "        \n",
    "        # Recurrent Module\n",
    "        out, (h_out, c_out) = self.lstm(x, (h,c))\n",
    "        \n",
    "        # Take the output from the last time step\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        # Classifier\n",
    "        out = self.classifier(out)\n",
    "        \n",
    "        return out\n",
    "    def init_state(self, b_size, device):\n",
    "        \"\"\" Initializing hidden and cell state \"\"\"\n",
    "        if(self.mode == \"zeros\"):\n",
    "            h = torch.zeros(self.num_layers, b_size, self.hidden_dim)\n",
    "            c = torch.zeros(self.num_layers, b_size, self.hidden_dim)\n",
    "        elif(self.mode == \"random\"):\n",
    "            h = torch.randn(self.num_layers, b_size, self.hidden_dim)\n",
    "            c = torch.randn(self.num_layers, b_size, self.hidden_dim)\n",
    "        elif(self.mode == \"learned\"):\n",
    "            h = self.learned_h.repeat(1, b_size, 1)\n",
    "            c = self.learned_c.repeat(1, b_size, 1)\n",
    "        h = h.to(device)\n",
    "        c = c.to(device)\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86f3dbc1-f7c7-4605-8b9a-14f21e34089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size, sequence_length, input_channels, height, width = 32, 20, 3, 64, 64\n",
    "num_classes = len(sequencesExtractor.get_classes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "214bc3e6-c062-4160-8434-ce233eb4c146",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = ConvRecurrentClassifier(input_channels=3, hidden_size=128, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18e970ba-de1a-42c7-8d2e-85b996ae331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()  # Note, that this already includes a Softmax!\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR) #adamW was used in the paper\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train_dataloader), epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59a1a7ad-d327-482d-a73d-43461f5268f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_model(model):\n",
    "    \"\"\" Computing model accuracy \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_list = []\n",
    "    \n",
    "    for sequences, labels in val_dataloader:\n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass only to get logits/output\n",
    "        outputs = model(sequences)\n",
    "                 \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "            \n",
    "        # Get predictions from the maximum value\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct += len( torch.where(preds==labels)[0] )\n",
    "        total += len(labels)\n",
    "                 \n",
    "    # Total correct predictions and loss\n",
    "    accuracy = correct / total * 100\n",
    "    loss = np.mean(loss_list)\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33439054-0afc-447d-83dc-eac9fd746f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iter 94: loss 1.50806. :  31%|███       | 94/308 [10:21<23:34,  6.61s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m  acc_list\u001b[38;5;241m.\u001b[39mappend(accuracy)\n\u001b[1;32m     27\u001b[0m  \u001b[38;5;66;03m# Getting gradients w.r.t. parameters\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m  \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0, norm_type=2.0)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m  \u001b[38;5;66;03m# Updating parameters\u001b[39;00m\n\u001b[1;32m     31\u001b[0m  optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/CudaLab/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/CudaLab/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#SAMPLE Training\n",
    "loss_hist = []\n",
    "valid_acc_hist = []\n",
    "for epoch in range(num_epochs):\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    progress_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader))\n",
    "    for i, (sequences, labels) in progress_bar:\n",
    "        sequences = sequences.to(device)\n",
    "        sequences = sequences.to(device)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(sequences)\n",
    "         \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predicted = outputs.argmax(dim=-1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            accuracy = correct/labels.shape[0] * 100\n",
    "        acc_list.append(accuracy)\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "       # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0, norm_type=2.0)\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_description(f\"Epoch {epoch+1} Iter {i+1}: loss {loss.item():.5f}. \")\n",
    "    \n",
    "    loss_hist.append(np.mean(loss_list))\n",
    "    train_acc_hist.append(np.mean(acc_list))\n",
    "    val_accuracy, valid_loss = eval_model(model)\n",
    "    print(f\"Val accuracy at epoch {epoch}: {round(val_accuracy, 2)}%\")\n",
    "    valid_loss_hist.append(valid_loss)\n",
    "    valid_acc_hist.append(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5f59aa-062a-4d2a-a0b5-1ddd0d8236fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
