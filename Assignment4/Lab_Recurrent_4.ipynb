{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ee6ddc65-0b5d-40e8-b7c7-986cf4d03055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pdb\n",
    "import numpy as np\n",
    "import yaml\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "436b1e3f-a3be-4ed7-bbec-560ab7b5519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the subjects for each split\n",
    "train_subjects = [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 3, 5, 6, 7, 10]\n",
    "val_subjects = [24, 25, 1, 4]\n",
    "test_subjects = [22, 2, 8, 9]\n",
    "\n",
    "# Define the background variations\n",
    "background_variations = ['d1', 'd2', 'd3', 'd4']\n",
    "\n",
    "processed_folder = './processed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bea475fe-d0bf-45b5-9538-9c0614a3ef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load config file and hyperparams\n",
    "config = yaml.safe_load(open(\"config.yaml\"))\n",
    "LR = float(config[\"LR\"])\n",
    "batch_size = int(config[\"BATCH_SIZE\"])\n",
    "num_epochs = int(config[\"NUM_EPOCHS\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52427c9c-27c0-4301-b52e-6c0d8cfcd917",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    # Add more augmentations if needed\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2a56bfe-c3af-4608-9019-caf7bc52dbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequencesExtractor:\n",
    "    def __init__(self, path, num_frames_per_subsequence=20):\n",
    "        # Define the number of frames per subsequence\n",
    "        self.num_frames_per_subsequence = num_frames_per_subsequence\n",
    "        # List all action folders in the processed folder\n",
    "        self.classes = os.listdir(path) # folders correspond to classes/labels\n",
    "        self.class_to_label = {class_name: idx for idx, class_name in enumerate(self.classes)}\n",
    "    \n",
    "    def get_classes(self):\n",
    "        return self.classes\n",
    "    def create_sequences(self, subjects, background_variations):\n",
    "        sequences = []\n",
    "        target_arr = []\n",
    "        # Iterate over each action folder\n",
    "        for action_folder in self.classes:\n",
    "            target = action_folder\n",
    "            action_path = os.path.join(processed_folder, action_folder)\n",
    "\n",
    "            # List all person folders in the action folder\n",
    "            person_folders_actual = os.listdir(action_path)\n",
    "            # Filter videos based on subjects and background variations\n",
    "            person_folders_target = [f'person{subject:02d}_{action_folder}_{bg}' \n",
    "                           for subject in subjects \n",
    "                           for bg in background_variations]\n",
    "            person_folders = set(person_folders_actual) & set(person_folders_target) # it can be that some background variations (or smth else) is missing. \n",
    "            # Iterate over each person folder\n",
    "            for person_folder in person_folders:\n",
    "                person_path = os.path.join(action_path, person_folder)\n",
    "                # List all image files in the person folder\n",
    "                image_files = sorted(os.listdir(person_path))\n",
    "\n",
    "                # Split the image files into subsequences\n",
    "                num_frames = len(image_files)\n",
    "                num_subsequences = num_frames // self.num_frames_per_subsequence\n",
    "\n",
    "                for i in range(num_subsequences):\n",
    "                    start_index = i * self.num_frames_per_subsequence\n",
    "                    end_index = start_index + self.num_frames_per_subsequence\n",
    "\n",
    "                    # Load and process the frames in the subsequence\n",
    "                    subsequence_frames = []\n",
    "                    for j in range(start_index, end_index):\n",
    "                        try:\n",
    "                            image_path = os.path.join(person_path, image_files[j])\n",
    "                            frame = Image.open(image_path).convert('RGB')\n",
    "                            # Apply any desired spatial augmentations to the frame\n",
    "                            frame = transform(frame)\n",
    "                            subsequence_frames.append(frame)\n",
    "                        except:\n",
    "                            print(\"Tried to read wrong file. Continuing\")\n",
    "                            continue\n",
    "                    # Apply any desired temporal augmentations to the subsequence\n",
    "\n",
    "                    # Process the subsequence (e.g., feed it to a model for action classification)\n",
    "                    subsequence_frames = torch.stack(subsequence_frames, dim=0)\n",
    "                    # Perform further processing on the subsequence\n",
    "\n",
    "                    target_arr.append(self.class_to_label[target])\n",
    "                    sequences.append(subsequence_frames)\n",
    "        return np.array(sequences), np.array(target_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f533d15-a84d-43a8-84ad-a38e23899726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to read wrong file. Continuing\n",
      "Tried to read wrong file. Continuing\n",
      "Tried to read wrong file. Continuing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-201df742e6b2>:60: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  return np.array(sequences), np.array(target_arr)\n",
      "<ipython-input-5-201df742e6b2>:60: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(sequences), np.array(target_arr)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried to read wrong file. Continuing\n",
      "Tried to read wrong file. Continuing\n",
      "Tried to read wrong file. Continuing\n",
      "Tried to read wrong file. Continuing\n",
      "Tried to read wrong file. Continuing\n",
      "Tried to read wrong file. Continuing\n"
     ]
    }
   ],
   "source": [
    "sequencesExtractor = SequencesExtractor(path=processed_folder, num_frames_per_subsequence=20)\n",
    "train_sequences, train_target_arr = sequencesExtractor.create_sequences(train_subjects, background_variations)\n",
    "test_sequences, test_target_arr = sequencesExtractor.create_sequences(test_subjects, background_variations)\n",
    "val_sequences, val_target_arr = sequencesExtractor.create_sequences(val_subjects, background_variations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ede534a-b8e5-4f6d-94c4-33d2830fc1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sequences length: 9844\n",
      "Validation sequences length: 2146\n",
      "Test sequences length: 2252\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training sequences length: {len(train_sequences)}\")\n",
    "print(f\"Validation sequences length: {len(val_sequences)}\")\n",
    "print(f\"Test sequences length: {len(test_sequences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c5cb3d4-bc76-458a-a340-35b92a077e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28561b37-ee02-4478-aabc-12375160948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KTHDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return sequence, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a513b5d-6569-4213-872d-e1982b6bdf14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9478965c-f0de-45e0-afd5-cc43f819cf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = KTHDataset(train_sequences, train_target_arr)\n",
    "test_dataset = KTHDataset(test_sequences, test_target_arr)\n",
    "val_dataset = KTHDataset(val_sequences, val_target_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a23c7c6e-4cb1-4889-ac54-456a848e0e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.KTHDataset at 0x7f9a2def10d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "641f7753-a710-4a5a-8167-15f91e1bab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d507ec-7265-495e-85b0-91906cd03275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "420430c4-617e-44db-a5d1-9af2636b4134",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Encapuslation of a convolutional block (conv + activation + pooling)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, k_size, pool=False):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(in_ch, out_ch, k_size))\n",
    "        layers.append(nn.ReLU())\n",
    "        if(pool):\n",
    "            layers.append(nn.MaxUnpool2d(kernel_size=2))\n",
    "        self.module = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return(self.module(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "41147836-3cb6-427f-9c02-0978314aee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvRecurrentClassifier(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_size, num_classes,num_layers = 1, mode=\"zeros\"):\n",
    "        assert mode in [\"zeros\", \"random\"]\n",
    "        super(ConvRecurrentClassifier, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.num_layers = 1\n",
    "        self.hidden_dim = hidden_size\n",
    "        #Convolutional Encoder\n",
    "        # self.conv_encoder = nn.Sequential(\n",
    "        #     ConvBlock(3, 16, 3, pool=False),\n",
    "        #     ConvBlock(16, 32, 3, pool=True),\n",
    "        #     ConvBlock(32, 64, 3, pool=False),\n",
    "        #     ConvBlock(64, 128, 3, pool=True)\n",
    "        # )\n",
    "        \n",
    "        self.conv_encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Add more convolutional layers if needed\n",
    "        )\n",
    "        \n",
    "        #Recurrent Module\n",
    "        self.lstm = nn.LSTM(input_size=128 * 16 * 16, hidden_size=hidden_size, batch_first=True)\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Reshape input to (batch_size * sequence_length, channels, height, width)\n",
    "        x = x.view(-1, x.size(2), x.size(3), x.size(4))\n",
    "        \n",
    "        h, c = self.init_state(b_size=batch_size, device=device)\n",
    "         # Convolutional Encoder\n",
    "        x = self.conv_encoder(x)\n",
    "        \n",
    "        # Reshape for LSTM\n",
    "        x = x.view(x.size(0), -1, 128 * 16 * 16)\n",
    "        \n",
    "        # Recurrent Module\n",
    "        out, (h_out, c_out) = self.lstm(x, (h,c))\n",
    "        \n",
    "        # Take the output from the last time step\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        # Classifier\n",
    "        out = self.classifier(out)\n",
    "        \n",
    "        return out\n",
    "    def init_state(self, b_size, device):\n",
    "        \"\"\" Initializing hidden and cell state \"\"\"\n",
    "        if(self.mode == \"zeros\"):\n",
    "            h = [torch.zeros(b_size, self.hidden_dim).to(device) for _ in range(self.num_layers)]\n",
    "            c = [torch.zeros(b_size, self.hidden_dim).to(device) for _ in range(self.num_layers)]\n",
    "        elif(self.mode == \"random\"):\n",
    "            h = [torch.zeros(b_size, self.hidden_dim).to(device) for _ in range(self.num_layers)]\n",
    "            c = [torch.zeros(b_size, self.hidden_dim).to(device) for _ in range(self.num_layers)]\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "86f3dbc1-f7c7-4605-8b9a-14f21e34089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size, sequence_length, input_channels, height, width = 32, 20, 3, 64, 64\n",
    "num_classes = len(sequencesExtractor.get_classes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "214bc3e6-c062-4160-8434-ce233eb4c146",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = ConvRecurrentClassifier(input_channels=3, hidden_size=128, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "18e970ba-de1a-42c7-8d2e-85b996ae331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()  # Note, that this already includes a Softmax!\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR) #adamW was used in the paper\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train_dataloader), epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "59a1a7ad-d327-482d-a73d-43461f5268f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_model(model):\n",
    "    \"\"\" Computing model accuracy \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_list = []\n",
    "    \n",
    "    for sequences, labels in val_dataloader:\n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass only to get logits/output\n",
    "        outputs = model(sequences)\n",
    "                 \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "            \n",
    "        # Get predictions from the maximum value\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct += len( torch.where(preds==labels)[0] )\n",
    "        total += len(labels)\n",
    "                 \n",
    "    # Total correct predictions and loss\n",
    "    accuracy = correct / total * 100\n",
    "    loss = np.mean(loss_list)\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "33439054-0afc-447d-83dc-eac9fd746f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/308 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Forward pass to get output/logits\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Calculate Loss: softmax --> cross entropy loss\u001b[39;00m\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[0;32m~/miniconda3/envs/CudaLab/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[71], line 46\u001b[0m, in \u001b[0;36mConvRecurrentClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m128\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m16\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Recurrent Module\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m out, (h_out, c_out) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Take the output from the last time step\u001b[39;00m\n\u001b[1;32m     49\u001b[0m out \u001b[38;5;241m=\u001b[39m out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
      "File \u001b[0;32m~/miniconda3/envs/CudaLab/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/CudaLab/lib/python3.8/site-packages/torch/nn/modules/rnn.py:795\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# If not PackedSequence input.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_batched:\n\u001b[0;32m--> 795\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\u001b[43mhx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m    796\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor batched 3-D input, hx and cx should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    797\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malso be 3-D but got (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D) tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    798\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "#SAMPLE Training\n",
    "loss_hist = []\n",
    "train_acc_hist = []\n",
    "valid_loss_hist = []\n",
    "valid_acc_hist = []\n",
    "for epoch in range(num_epochs):\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    progress_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader))\n",
    "    for i, (sequences, labels) in progress_bar:\n",
    "        sequences = sequences.to(device)\n",
    "        sequences = sequences.to(device)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "         \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predicted = outputs.argmax(dim=-1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            accuracy = correct/labels.shape[0] * 100\n",
    "        acc_list.append(accuracy)\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "       # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0, norm_type=2.0)\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_description(f\"Epoch {epoch+1} Iter {i+1}: loss {loss.item():.5f}. \")\n",
    "    \n",
    "    loss_hist.append(np.mean(loss_list))\n",
    "    train_acc_hist.append(np.mean(acc_list))\n",
    "    val_accuracy, valid_loss = eval_model(model)\n",
    "    print(f\"Val accuracy at epoch {epoch}: {round(val_accuracy, 2)}%\")\n",
    "    valid_loss_hist.append(valid_loss)\n",
    "    valid_acc_hist.append(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5f59aa-062a-4d2a-a0b5-1ddd0d8236fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
