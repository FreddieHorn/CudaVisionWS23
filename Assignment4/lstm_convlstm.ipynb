{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee6ddc65-0b5d-40e8-b7c7-986cf4d03055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pdb\n",
    "import numpy as np\n",
    "import yaml\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "436b1e3f-a3be-4ed7-bbec-560ab7b5519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the subjects for each split\n",
    "train_subjects = [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 3, 5, 6, 7, 10]\n",
    "val_subjects = [24, 25, 1, 4]\n",
    "test_subjects = [22, 2, 8, 9]\n",
    "\n",
    "# Define the background variations\n",
    "background_variations = ['d1', 'd2', 'd3', 'd4']\n",
    "\n",
    "processed_folder = '/home/nfs/inf6/data/datasets/kth_actions/processed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bea475fe-d0bf-45b5-9538-9c0614a3ef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load config file and hyperparams\n",
    "config = yaml.safe_load(open(\"config.yaml\"))\n",
    "LR = float(config[\"LR\"])\n",
    "batch_size = int(config[\"BATCH_SIZE\"])\n",
    "num_epochs = int(config[\"NUM_EPOCHS\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52427c9c-27c0-4301-b52e-6c0d8cfcd917",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    # Add more augmentations if needed\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2a56bfe-c3af-4608-9019-caf7bc52dbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequencesExtractor:\n",
    "    def __init__(self, path, num_frames_per_subsequence=20):\n",
    "        # Define the number of frames per subsequence\n",
    "        self.num_frames_per_subsequence = num_frames_per_subsequence\n",
    "        # List all action folders in the processed folder\n",
    "        self.classes = os.listdir(path) # folders correspond to classes/labels\n",
    "        self.class_to_label = {class_name: idx for idx, class_name in enumerate(self.classes)}\n",
    "    \n",
    "    def get_classes(self):\n",
    "        return self.classes\n",
    "    def create_sequences(self, subjects, background_variations):\n",
    "        sequences = []\n",
    "        target_arr = []\n",
    "        # Iterate over each action folder\n",
    "        for action_folder in self.classes:\n",
    "            target = action_folder\n",
    "            action_path = os.path.join(processed_folder, action_folder)\n",
    "\n",
    "            # List all person folders in the action folder\n",
    "            person_folders_actual = os.listdir(action_path)\n",
    "            # Filter videos based on subjects and background variations\n",
    "            person_folders_target = [f'person{subject:02d}_{action_folder}_{bg}' \n",
    "                           for subject in subjects \n",
    "                           for bg in background_variations]\n",
    "            person_folders = set(person_folders_actual) & set(person_folders_target) # it can be that some background variations (or smth else) is missing. \n",
    "            # Iterate over each person folder\n",
    "            for person_folder in person_folders:\n",
    "                person_path = os.path.join(action_path, person_folder)\n",
    "                # List all image files in the person folder\n",
    "                image_files = sorted(os.listdir(person_path))\n",
    "\n",
    "                # Split the image files into subsequences\n",
    "                num_frames = len(image_files)\n",
    "                num_subsequences = num_frames // self.num_frames_per_subsequence\n",
    "\n",
    "                for i in range(num_subsequences):\n",
    "                    start_index = i * self.num_frames_per_subsequence\n",
    "                    end_index = start_index + self.num_frames_per_subsequence\n",
    "\n",
    "                    # Load and process the frames in the subsequence\n",
    "                    subsequence_frames = []\n",
    "                    for j in range(start_index, end_index):\n",
    "                        try:\n",
    "                            image_path = os.path.join(person_path, image_files[j])\n",
    "                            frame = Image.open(image_path).convert('RGB')\n",
    "                            # Apply any desired spatial augmentations to the frame\n",
    "                            frame = transform(frame)\n",
    "                            subsequence_frames.append(frame)\n",
    "                        except:\n",
    "                            print(\"Tried to read wrong file. Continuing\")\n",
    "                            continue\n",
    "                    # Apply any desired temporal augmentations to the subsequence\n",
    "\n",
    "                    # Process the subsequence (e.g., feed it to a model for action classification)\n",
    "                    subsequence_frames = torch.stack(subsequence_frames, dim=0)\n",
    "                    # Perform further processing on the subsequence\n",
    "\n",
    "                    target_arr.append(self.class_to_label[target])\n",
    "                    sequences.append(subsequence_frames)\n",
    "        return np.array(sequences), np.array(target_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f533d15-a84d-43a8-84ad-a38e23899726",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_folder = '/home/nfs/inf6/data/datasets/kth_actions/processed/'\n",
    "sequencesExtractor = SequencesExtractor(path=processed_folder, num_frames_per_subsequence=20)\n",
    "train_sequences, train_target_arr = sequencesExtractor.create_sequences(train_subjects, background_variations)\n",
    "test_sequences, test_target_arr = sequencesExtractor.create_sequences(test_subjects, background_variations)\n",
    "val_sequences, val_target_arr = sequencesExtractor.create_sequences(val_subjects, background_variations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ede534a-b8e5-4f6d-94c4-33d2830fc1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sequences length: 9844\n",
      "Validation sequences length: 2146\n",
      "Test sequences length: 2252\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training sequences length: {len(train_sequences)}\")\n",
    "print(f\"Validation sequences length: {len(val_sequences)}\")\n",
    "print(f\"Test sequences length: {len(test_sequences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9844, 20, 3, 64, 64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c5cb3d4-bc76-458a-a340-35b92a077e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28561b37-ee02-4478-aabc-12375160948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KTHDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return sequence, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a513b5d-6569-4213-872d-e1982b6bdf14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9478965c-f0de-45e0-afd5-cc43f819cf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = KTHDataset(train_sequences, train_target_arr)\n",
    "test_dataset = KTHDataset(test_sequences, test_target_arr)\n",
    "val_dataset = KTHDataset(val_sequences, val_target_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a23c7c6e-4cb1-4889-ac54-456a848e0e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.KTHDataset at 0x7efe60ac4520>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "641f7753-a710-4a5a-8167-15f91e1bab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class OurLSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(OurLSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Input gate\n",
    "        self.W_ii = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.W_hi = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.b_ii = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.b_hi = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        # Forget gate\n",
    "        self.W_if = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.W_hf = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.b_if = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.b_hf = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        # Cell gate\n",
    "        self.W_ig = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.W_hg = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.b_ig = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.b_hg = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        # Output gate\n",
    "        self.W_io = nn.Parameter(torch.Tensor(hidden_size, input_size))\n",
    "        self.W_ho = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.b_io = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.b_ho = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            # dimension greater equal to 2 is typically associated with weight tensors\n",
    "            if p.data.ndimension() >= 2:\n",
    "                nn.init.xavier_uniform_(p.data)\n",
    "            else:\n",
    "            # bias tensors are commonly initialized to 0\n",
    "                nn.init.zeros_(p.data)\n",
    "    \n",
    "    \n",
    "    def forward(self, x, init_states=None):\n",
    "        # bs, _ = x.size()\n",
    "\n",
    "        # Access prior hidden state if existent, else initialize to zero (i.e. future is unknown)\n",
    "        h_t, c_t = (torch.zeros(self.hidden_size).to(x.device),\n",
    "                    torch.zeros(self.hidden_size).to(x.device)) if init_states is None else init_states\n",
    "\n",
    "        # We note: In a perfect world, this should be parallelized\n",
    "        # Input gate\n",
    "        i_t = torch.sigmoid(x @ self.W_ii.t() + self.b_ii + h_t @ self.W_hi.t() + self.b_hi)\n",
    "        \n",
    "        # Forget gate\n",
    "        f_t = torch.sigmoid(x @ self.W_if.t() + self.b_if + h_t @ self.W_hf.t() + self.b_hf)\n",
    "        \n",
    "        # Cell gate\n",
    "        g_t = torch.tanh(x @ self.W_ig.t() + self.b_ig + h_t @ self.W_hg.t() + self.b_hg)\n",
    "        \n",
    "        # Output gate\n",
    "        o_t = torch.sigmoid(x @ self.W_io.t() + self.b_io + h_t @ self.W_ho.t() + self.b_ho)\n",
    "        \n",
    "        # Note: * is the Hadamard product\n",
    "        # Update cell state\n",
    "        c_t = f_t * c_t + i_t * g_t\n",
    "\n",
    "        # Update hidden state\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "        return h_t, c_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchcell = nn.LSTMCell(200, 100)\n",
    "# torchcell(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = OurLSTMCell(200, 100)\n",
    "input = torch.ones(10, 200)\n",
    "\n",
    " #cell(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note, that we omitted the hadamard products from the paper to stay more similar to our LSTM implementation from above\n",
    "https://arxiv.org/pdf/1506.04214v2.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ConvLSTMCell' object has no attribute 'hidden_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/user/lschulze/projects/CudaVisionWS23/Assignment4/Lab_Recurrent_4 copy.ipynb Cell 19\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcuda6.informatik.uni-bonn.de/home/user/lschulze/projects/CudaVisionWS23/Assignment4/Lab_Recurrent_4%20copy.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m conv_lstm_cell \u001b[39m=\u001b[39m ConvLSTMCell(input_size \u001b[39m=\u001b[39m (\u001b[39m32\u001b[39m, \u001b[39m64\u001b[39m, \u001b[39m64\u001b[39m), channels\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcuda6.informatik.uni-bonn.de/home/user/lschulze/projects/CudaVisionWS23/Assignment4/Lab_Recurrent_4%20copy.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39m# Random input sequence\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcuda6.informatik.uni-bonn.de/home/user/lschulze/projects/CudaVisionWS23/Assignment4/Lab_Recurrent_4%20copy.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcuda6.informatik.uni-bonn.de/home/user/lschulze/projects/CudaVisionWS23/Assignment4/Lab_Recurrent_4%20copy.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcuda6.informatik.uni-bonn.de/home/user/lschulze/projects/CudaVisionWS23/Assignment4/Lab_Recurrent_4%20copy.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m output_seq, final_states \u001b[39m=\u001b[39m conv_lstm_cell(input_seq)\n",
      "File \u001b[0;32m~/anaconda3/envs/lab2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/lab2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/user/lschulze/projects/CudaVisionWS23/Assignment4/Lab_Recurrent_4 copy.ipynb Cell 19\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcuda6.informatik.uni-bonn.de/home/user/lschulze/projects/CudaVisionWS23/Assignment4/Lab_Recurrent_4%20copy.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, init_states\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcuda6.informatik.uni-bonn.de/home/user/lschulze/projects/CudaVisionWS23/Assignment4/Lab_Recurrent_4%20copy.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m     bs, channels, rows, cols \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcuda6.informatik.uni-bonn.de/home/user/lschulze/projects/CudaVisionWS23/Assignment4/Lab_Recurrent_4%20copy.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m     h_t, c_t \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhidden_size, x\u001b[39m.\u001b[39msize(\u001b[39m2\u001b[39m), x\u001b[39m.\u001b[39msize(\u001b[39m3\u001b[39m))\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdevice),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcuda6.informatik.uni-bonn.de/home/user/lschulze/projects/CudaVisionWS23/Assignment4/Lab_Recurrent_4%20copy.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m                 torch\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size, x\u001b[39m.\u001b[39msize(\u001b[39m2\u001b[39m), x\u001b[39m.\u001b[39msize(\u001b[39m3\u001b[39m))\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdevice)) \u001b[39mif\u001b[39;00m init_states \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m init_states\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcuda6.informatik.uni-bonn.de/home/user/lschulze/projects/CudaVisionWS23/Assignment4/Lab_Recurrent_4%20copy.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m     \u001b[39m# Input gate\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcuda6.informatik.uni-bonn.de/home/user/lschulze/projects/CudaVisionWS23/Assignment4/Lab_Recurrent_4%20copy.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m     i_t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msigmoid(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW_ii(x) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb_i \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW_hi(h_t))\n",
      "File \u001b[0;32m~/anaconda3/envs/lab2/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ConvLSTMCell' object has no attribute 'hidden_size'"
     ]
    }
   ],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, channels, input_size, kernel_size = 3):\n",
    "        '''\n",
    "        Params:\n",
    "\n",
    "        channels: number of channels, which is used as input and output\n",
    "        input_size: The input size of the image\n",
    "        kernel_size: size of the convolutional kernel\n",
    "        '''\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        # Input gate\n",
    "        self.W_ii = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.W_hi = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.b_i = nn.Parameter(torch.zeros(input_size))\n",
    "\n",
    "        # Forget gate\n",
    "        self.W_if = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.W_hf = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.b_f = nn.Parameter(torch.zeros(input_size))\n",
    "\n",
    "        # Cell gate\n",
    "        self.W_ig = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.W_hg = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.b_g = nn.Parameter(torch.zeros(input_size))\n",
    "\n",
    "        # Output gate\n",
    "        self.W_io = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.W_ho = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.b_o = nn.Parameter(torch.zeros(input_size))\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    # Same as above\n",
    "    def init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.data.ndimension() >= 2:\n",
    "                nn.init.xavier_uniform_(p.data)\n",
    "            else:\n",
    "                nn.init.zeros_(p.data)\n",
    "\n",
    "    def forward(self, x, init_states=None):\n",
    "        bs, channels, rows, cols = x.size()\n",
    "\n",
    "        h_t, c_t = (torch.zeros(self.input_size).to(x.device),\n",
    "                    torch.zeros(self.input_size)) if init_states is None else init_states\n",
    "\n",
    "        \n",
    "        # Input gate\n",
    "        i_t = torch.sigmoid(self.W_ii(x) + self.b_i + self.W_hi(h_t))\n",
    "        \n",
    "        # Forget gate\n",
    "        f_t = torch.sigmoid(self.W_if(x) + self.W_hf(h_t) + self.b_f)\n",
    "        \n",
    "        # Cell gate\n",
    "        g_t = torch.tanh(self.W_ig(x) + self.b_g + self.W_hg(h_t))\n",
    "        \n",
    "        # Output gate\n",
    "        o_t = torch.sigmoid(self.W_io(x) + self.b_o + self.W_ho(h_t))\n",
    "        \n",
    "        # Update cell state\n",
    "        c_t = f_t * c_t + i_t * g_t\n",
    "\n",
    "        # Update hidden state\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "        return h_t, c_t\n",
    "\n",
    "\n",
    "\n",
    "input_seq = torch.randn(128, 32, 64, 64)\n",
    "\n",
    "\n",
    "conv_lstm_cell = ConvLSTMCell(input_size = (32, 64, 64), channels=32)\n",
    "\n",
    "# Random input sequence\n",
    "\n",
    "# Forward pass\n",
    "output_seq, final_states = conv_lstm_cell(input_seq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct LSTM from scratch\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm_cells = nn.ModuleList([LSTMCell(input_size, hidden_size) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, init_states=None):\n",
    "        bs, seq_len, _ = x.size()\n",
    "\n",
    "        h_t, c_t = (torch.zeros(self.num_layers, self.hidden_size).to(x.device),\n",
    "                    torch.zeros(self.num_layers, self.hidden_size).to(x.device)) if init_states is None else init_states\n",
    "\n",
    "        h_t, c_t = h_t.clone(), c_t.clone()\n",
    "\n",
    "        output = []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :]\n",
    "            for layer in range(self.num_layers):\n",
    "                h_t[layer], c_t[layer] = self.lstm_cells[layer](x_t, (h_t[layer], c_t[layer]))\n",
    "                x_t = h_t[layer]\n",
    "\n",
    "            output.append(x_t)\n",
    "\n",
    "        output = torch.stack(output, dim=1)\n",
    "\n",
    "        return output, (h_t, c_t)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "num_layers = 2\n",
    "seq_len = 5\n",
    "batch_size = 3\n",
    "\n",
    "lstm = LSTM(input_size, hidden_size, num_layers)\n",
    "\n",
    "# Random input sequence\n",
    "input_seq = torch.randn(batch_size, seq_len, input_size)\n",
    "\n",
    "# Forward pass\n",
    "output_seq, final_states = lstm(input_seq)\n",
    "\n",
    "print(\"Input sequence shape:\", input_seq.shape)\n",
    "print(\"Output sequence shape:\", output_seq.shape)\n",
    "print(\"Final hidden state shape:\", final_states[0].shape)\n",
    "print(\"Final cell state shape:\", final_states[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d507ec-7265-495e-85b0-91906cd03275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMbyHand(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "420430c4-617e-44db-a5d1-9af2636b4134",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Encapuslation of a convolutional block (conv + activation + pooling)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, k_size, pool=False):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(in_ch, out_ch, k_size))\n",
    "        layers.append(nn.ReLU())\n",
    "        if(pool):\n",
    "            layers.append(nn.MaxPool2d(kernel_size=2))\n",
    "        self.module = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return(self.module(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41147836-3cb6-427f-9c02-0978314aee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvRecurrentClassifier(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_size, num_classes,num_layers = 1, mode=\"zeros\"):\n",
    "        assert mode in [\"zeros\", \"random\"]\n",
    "        super(ConvRecurrentClassifier, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.num_layers = 1\n",
    "        self.hidden_dim = hidden_size\n",
    "        #Convolutional Encoder\n",
    "        # self.conv_encoder = nn.Sequential(\n",
    "        #     ConvBlock(3, 16, 3, pool=False),\n",
    "        #     ConvBlock(16, 32, 3, pool=True),\n",
    "        #     ConvBlock(32, 64, 3, pool=False),\n",
    "        #     ConvBlock(64, 128, 3, pool=True)\n",
    "        # )\n",
    "        \n",
    "        self.conv_encoder = nn.Sequential(\n",
    "            # nn.Conv2d(input_channels, 64, kernel_size=3, padding=1),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Add more convolutional layers if needed\n",
    "            ConvBlock(3, 16, 3, pool=False),\n",
    "            ConvBlock(16, 32, 3, pool=True),\n",
    "            ConvBlock(32, 64, 3, pool=False),\n",
    "            ConvBlock(64, 128, 3, pool=True),\n",
    "            ConvBlock(128, 256, 3, pool=False)\n",
    "        )\n",
    "        \n",
    "        #Recurrent Module\n",
    "        self.lstm = nn.LSTM(input_size=256 * 315 * 11, hidden_size=hidden_size, batch_first=True)\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Reshape input to (batch_size * sequence_length, channels, height, width)\n",
    "        b_size, seq_length, n_channels, width, height = x.shape\n",
    "        \n",
    "        x = x.view(b_size, n_channels, seq_length*width, height)\n",
    "        \n",
    "        h, c = self.init_state(b_size=batch_size, device=device)\n",
    "         # Convolutional Encoder\n",
    "        x = self.conv_encoder(x)\n",
    "        \n",
    "        # Reshape for LSTM\n",
    "        x = x.view(x.size(0), -1, x.size(1) * x.size(2) * x.size(3))\n",
    "        \n",
    "        # Recurrent Module\n",
    "        out, (h_out, c_out) = self.lstm(x, (h,c))\n",
    "        \n",
    "        # Take the output from the last time step\n",
    "        out = out[:, -1, :]\n",
    "        \n",
    "        # Classifier\n",
    "        out = self.classifier(out)\n",
    "        \n",
    "        return out\n",
    "    def init_state(self, b_size, device):\n",
    "        \"\"\" Initializing hidden and cell state \"\"\"\n",
    "        if(self.mode == \"zeros\"):\n",
    "            h = torch.zeros(self.num_layers, b_size, self.hidden_dim)\n",
    "            c = torch.zeros(self.num_layers, b_size, self.hidden_dim)\n",
    "        elif(self.mode == \"random\"):\n",
    "            h = torch.randn(self.num_layers, b_size, self.hidden_dim)\n",
    "            c = torch.randn(self.num_layers, b_size, self.hidden_dim)\n",
    "        elif(self.mode == \"learned\"):\n",
    "            h = self.learned_h.repeat(1, b_size, 1)\n",
    "            c = self.learned_c.repeat(1, b_size, 1)\n",
    "        h = h.to(device)\n",
    "        c = c.to(device)\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86f3dbc1-f7c7-4605-8b9a-14f21e34089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size, sequence_length, input_channels, height, width = 32, 20, 3, 64, 64\n",
    "num_classes = len(sequencesExtractor.get_classes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "214bc3e6-c062-4160-8434-ce233eb4c146",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = ConvRecurrentClassifier(input_channels=3, hidden_size=128, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18e970ba-de1a-42c7-8d2e-85b996ae331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()  # Note, that this already includes a Softmax!\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR) #adamW was used in the paper\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train_dataloader), epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59a1a7ad-d327-482d-a73d-43461f5268f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_model(model):\n",
    "    \"\"\" Computing model accuracy \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_list = []\n",
    "    \n",
    "    for sequences, labels in val_dataloader:\n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass only to get logits/output\n",
    "        outputs = model(sequences)\n",
    "                 \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "            \n",
    "        # Get predictions from the maximum value\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct += len( torch.where(preds==labels)[0] )\n",
    "        total += len(labels)\n",
    "                 \n",
    "    # Total correct predictions and loss\n",
    "    accuracy = correct / total * 100\n",
    "    loss = np.mean(loss_list)\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33439054-0afc-447d-83dc-eac9fd746f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iter 94: loss 1.50806. :  31%|███       | 94/308 [10:21<23:34,  6.61s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m  acc_list\u001b[38;5;241m.\u001b[39mappend(accuracy)\n\u001b[1;32m     27\u001b[0m  \u001b[38;5;66;03m# Getting gradients w.r.t. parameters\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m  \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0, norm_type=2.0)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m  \u001b[38;5;66;03m# Updating parameters\u001b[39;00m\n\u001b[1;32m     31\u001b[0m  optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/CudaLab/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/CudaLab/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#SAMPLE Training\n",
    "loss_hist = []\n",
    "valid_acc_hist = []\n",
    "for epoch in range(num_epochs):\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    progress_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader))\n",
    "    for i, (sequences, labels) in progress_bar:\n",
    "        sequences = sequences.to(device)\n",
    "        sequences = sequences.to(device)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(sequences)\n",
    "         \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predicted = outputs.argmax(dim=-1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            accuracy = correct/labels.shape[0] * 100\n",
    "        acc_list.append(accuracy)\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "       # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0, norm_type=2.0)\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_description(f\"Epoch {epoch+1} Iter {i+1}: loss {loss.item():.5f}. \")\n",
    "    \n",
    "    loss_hist.append(np.mean(loss_list))\n",
    "    train_acc_hist.append(np.mean(acc_list))\n",
    "    val_accuracy, valid_loss = eval_model(model)\n",
    "    print(f\"Val accuracy at epoch {epoch}: {round(val_accuracy, 2)}%\")\n",
    "    valid_loss_hist.append(valid_loss)\n",
    "    valid_acc_hist.append(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5f59aa-062a-4d2a-a0b5-1ddd0d8236fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
